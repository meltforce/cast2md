# cast2md - Project Knowledge

## Deployment

The production server runs on `<server>` (Tailscale hostname).

**Important:** Always commit, push, and deploy before testing any server-relevant code changes. The server runs from the git repository, not from your local files.

To deploy:
```bash
git add -A && git commit -m "Your message" && git push
ssh root@<server> "cd /opt/cast2md && git pull && systemctl restart cast2md"
```

## Architecture

- **Server**: Runs on the server via systemd (`systemctl restart cast2md`)
- **Node workers**: Remote transcription nodes connect to the server
- **Local workers**: Download workers and one local transcription worker run on the server
- **Database**: PostgreSQL with pgvector, runs in Docker (`docker compose up -d postgres`)

### Production Database

PostgreSQL runs as a Docker container on the server:

```bash
# Start PostgreSQL (required before cast2md service)
cd /opt/cast2md && docker compose up -d postgres

# Check status
docker ps | grep postgres
```

The container uses `pgvector/pgvector:pg16` image with data persisted in a Docker volume (`postgres_data`).

## Database Selection

**Important lesson learned:** If you encounter SQLite "database is locked" errors, migrate to PostgreSQL immediately. Don't spend time trying to optimize SQLite concurrency (WAL mode, busy timeouts, reducing workers) - it's a fundamental limitation.

Signs you need PostgreSQL:
- "database is locked" errors in logs
- Workers showing erratic counts (e.g., 3/10 running when 10 configured)
- Jobs getting stuck or timing out due to lock contention

PostgreSQL migration takes ~2 hours and eliminates all lock issues. With PostgreSQL, 10 workers can process ~10 transcripts/second with zero lock errors. See `docs/database-implementation.md` for details.

## Development

- Status UI: https://<your-tailnet>/status
- API docs: https://<your-tailnet>/docs

## Documentation

- `cast2md-requirements.md` - Central requirements document with architecture, data model, and development phases
- `docs/` - Additional documentation (distributed transcription setup, architecture diagrams, etc.)

## Testing


### API Testing

Always test the server API directly via the URL, not by SSH + localhost:
```bash
# Good - direct API call
curl https://<your-tailnet>/api/health

# Bad - unnecessary SSH
ssh root@<server> "curl localhost:8000/api/health"
```

## Transcription

### Backends

The system supports two transcription backends:

| Backend | Use Case | Languages | Speed |
|---------|----------|-----------|-------|
| **Whisper** | Local/server transcription | 99+ languages | Varies by model |
| **Parakeet** | RunPod GPU pods (default) | 25 EU languages | Very fast |

The backend is controlled by `TRANSCRIPTION_BACKEND` environment variable (`whisper` or `parakeet`).

### Model Tracking

Episodes track which model was used via `transcript_model` column (e.g., `parakeet-tdt-0.6b-v3`, `large-v3-turbo`). This is visible on the episode detail page.

### Re-transcription API

API endpoints exist for script-based re-transcription (UI removed):
- `GET /api/queue/retranscribe/info/{feed_id}` - get current model and count
- `POST /api/queue/episodes/{id}/retranscribe` - queue single episode
- `POST /api/queue/batch/feed/{id}/retranscribe` - queue all outdated in feed

## iTunes URL Support

Feeds can be added via Apple Podcasts URLs. The system automatically resolves them to RSS feed URLs.

### How It Works

1. `feed/itunes.py:resolve_feed_url()` detects Apple Podcasts URLs
2. Extracts iTunes ID from URL pattern `podcasts.apple.com/.*/id(\d+)`
3. Calls iTunes Lookup API to get RSS feed URL
4. Stores `itunes_id` on the feed for reference

### Key Files

- `clients/itunes.py` - iTunes API client (`ItunesClient.lookup()`)
- `feed/itunes.py` - URL detection and resolution
- `api/feeds.py:create_feed()` - Calls `resolve_feed_url()` before validation

## Transcript Sources

Episodes track where their transcript came from via `transcript_source` column:

- `whisper` - Self-transcribed using Whisper
- `podcast2.0:vtt` - Downloaded from publisher (WebVTT format)
- `podcast2.0:srt` - Downloaded from publisher (SRT format)
- `podcast2.0:json` - Downloaded from publisher (JSON format)
- `podcast2.0:text` - Downloaded from publisher (plain text)
- `pocketcasts` - Auto-generated by Pocket Casts
- `NULL` - Legacy episodes (before this feature)

### Transcript-First Workflow

When a feed is added or refreshed, the system queues `TRANSCRIPT_DOWNLOAD` jobs:

1. `feed/discovery.py:discover_new_episodes()` queues `TRANSCRIPT_DOWNLOAD` jobs (not `DOWNLOAD`)
2. `worker/manager.py:_process_transcript_download_job()` tries external providers
3. If transcript found: saves it, marks episode `COMPLETED` (no audio needed)
4. If not found: episode stays `PENDING` for manual audio download

This is storage-efficient - audio is only downloaded when transcripts aren't available externally.

### Provider Priority

1. **Podcast20Provider** - RSS `<podcast:transcript>` tags (authoritative)
2. **PocketCastsProvider** - Auto-generated transcripts (fallback)
3. **Whisper** - Self-transcription (only after audio download)

### Pocket Casts Provider

Uses public Pocket Casts API (no authentication required):

1. Search API: `POST podcast-api.pocketcasts.com/discover/search`
2. Show notes API: `GET podcast-api.pocketcasts.com/mobile/show_notes/full/{uuid}`
   - Returns redirect to static JSON, must follow redirects
3. Downloads from `pocket_casts_transcripts[]` array (VTT format)

The provider:
- Searches by feed title, matches by author
- Caches `pocketcasts_uuid` on feed after first successful search
- Matches episodes by title similarity + published date within 24h

### Adding New Providers

1. Create `src/cast2md/transcription/providers/newprovider.py`
2. Implement `TranscriptProvider` base class:
   - `source_id` property (e.g., `"newprovider"`)
   - `can_provide(episode, feed)` - check if provider applies
   - `fetch(episode, feed)` - download and return `TranscriptResult`
3. Register in `providers/__init__.py`:
   ```python
   _providers = [
       Podcast20Provider(),
       PocketCastsProvider(),
       NewProvider(),  # Add here
   ]
   ```

### Key Files

- `clients/pocketcasts.py` - Pocket Casts API client
- `transcription/providers/base.py` - `TranscriptProvider` abstract base class
- `transcription/providers/podcast20.py` - Podcasting 2.0 implementation
- `transcription/providers/pocketcasts.py` - Pocket Casts implementation
- `transcription/providers/__init__.py` - Provider registry and `try_fetch_transcript()`
- `transcription/formats.py` - VTT/SRT/JSON/text parsers
- `worker/manager.py:_process_transcript_download_job()` - Transcript download handler
- `worker/manager.py:_queue_transcription()` - Post-download transcription (Whisper fallback)

## Web UI Workflow

### Feed Episode List (feed_detail.html)

The episode list uses a **transcript-first** approach with real-time status updates:

#### Button Behavior

| Episode Status | Button | Action |
|----------------|--------|--------|
| `pending` | "Get Transcript" | Queues `TRANSCRIPT_DOWNLOAD` job |
| `downloaded` | "Transcribe" | Queues `TRANSCRIBE` job (Whisper) |
| `failed` | "Retry" | Queues `DOWNLOAD` job |
| `downloading`, `transcribing`, `queued` | "..." (disabled) | No action, status shown in badge |
| `completed` | (none) | Link to episode detail |

#### Transcript Download Flow

1. User clicks "Get Transcript" → `POST /api/queue/episodes/{id}/transcript-download`
2. Button becomes disabled ("..."), status badge shows "queued"
3. Worker tries Podcast20Provider, then PocketCastsProvider
4. If found: episode marked `COMPLETED`, button becomes link to detail
5. If not found: episode returns to `PENDING`, button shows "Download Audio"

When transcript download fails (no external transcript available), the button changes to "Download Audio" which queues the full audio download + Whisper transcription pipeline.

#### Real-time Status Updates

The feed page polls `/api/feeds/{id}/episodes` every 2 seconds while jobs are in progress:

- `startStatusPolling()` - Starts interval timer
- `stopStatusPolling()` - Stops when all visible episodes are completed/failed/pending
- `pollEpisodeStatus()` - Fetches current status and updates DOM
- `updateEpisodeRow()` - Updates badge, checkbox, and action button

Polling uses visible episode IDs from DOM (not template-rendered array) to handle pagination correctly.

#### Batch Operations

- "Get All Transcripts" button queues all pending episodes via `POST /api/queue/batch/feed/{id}/transcript-download`

### Episode Detail Page (episode_detail.html)

Shows full episode info with transcript viewer and manual action buttons:

| Status | Available Actions |
|--------|-------------------|
| `pending` | "Try Transcript Download", "Download Audio" |
| `downloaded` | "Queue Transcription" |
| `completed` | "Delete Audio" (if audio exists), "Download Audio" (if deleted) |
| `failed` | "Retry" |

### Queue API Endpoints

| Endpoint | Description |
|----------|-------------|
| `POST /api/queue/episodes/{id}/process` | Download audio (creates `DOWNLOAD` job) |
| `POST /api/queue/episodes/{id}/transcribe` | Whisper transcription (creates `TRANSCRIBE` job) |
| `POST /api/queue/episodes/{id}/transcript-download` | Try external providers (creates `TRANSCRIPT_DOWNLOAD` job) |
| `POST /api/queue/episodes/{id}/retranscribe` | Re-transcribe with current model |
| `POST /api/queue/batch/feed/{id}/transcript-download` | Batch transcript download for all pending |
| `POST /api/queue/batch/feed/{id}/retranscribe` | Batch re-transcribe for outdated episodes |

## Audio Management

Episodes with external transcripts don't need audio files. The audio can be deleted to save space:

- `DELETE /api/episodes/{id}/audio` - Deletes audio file, keeps `audio_url` for re-download
- Only allowed if episode has a transcript
- Episode detail page shows "Delete Audio" / "Download Audio" buttons accordingly

## Feed Deletion and Trash

When a feed is deleted, files are moved to trash instead of being permanently deleted:

### How It Works

1. User clicks "Delete Feed" on feed detail page
2. Confirmation dialog requires typing "delete"
3. `DELETE /api/feeds/{id}` moves files to trash, then deletes DB records
4. Server auto-cleans trash entries older than 30 days on startup

### Trash Structure

```
{storage_path}/trash/{feed_slug}_{feed_id}_{timestamp}/
├── audio/
│   └── {feed_id}/
│       └── *.mp3
└── transcripts/
    └── {feed_id}/
        └── *.json
```

### Key Files

- `storage/filesystem.py` - `move_feed_to_trash()`, `cleanup_old_trash()`
- `api/feeds.py:delete_feed()` - Calls trash functions before DB deletion
- `main.py:lifespan()` - Runs cleanup on server startup

### Limitations

- DB records are deleted immediately (no restore from trash)
- Only files are preserved in trash
- Manual restore requires re-adding feed and copying files back

## Semantic Search

Semantic search enables natural language queries like "protein and muscle building" to find conceptually related content across transcripts, even when exact keywords don't match.

### How It Works

1. **Hybrid Search**: Combines PostgreSQL full-text search with vector similarity using Reciprocal Rank Fusion (RRF)
2. **Embeddings**: Uses `sentence-transformers` with multilingual model (384-dim, ~470MB)
3. **Vector Storage**: pgvector extension with HNSW index for fast approximate nearest neighbor search

### Embedding Model

Uses `paraphrase-multilingual-MiniLM-L12-v2` for German language support:
- 50+ languages including German
- Understands semantic similarity (e.g., "kaltbaden" ≈ "eisbaden")
- 384 dimensions, ~470MB model size
- Configured in `search/embeddings.py`

### Segment Merging

Transcripts (both Whisper and external) can have word-level timestamps where each word is a separate segment. The system automatically merges these into phrases:

- Merging happens during indexing (FTS and embeddings) and display
- Phrase boundaries: punctuation, pauses (>1.5s), or max 200 chars
- Improves both search quality (fewer noisy results) and readability
- See `search/parser.py:merge_word_level_segments()`

### Architecture

```
Query → Generate embedding (~20ms)
     ↓
     ├── PostgreSQL tsvector search (fast)
     └── pgvector HNSW search (fast)
     ↓
     RRF fusion → Combined results
```

### Key Files

- `search/embeddings.py` - Embedding generation, model config
- `search/parser.py` - `merge_word_level_segments()` for phrase merging
- `search/repository.py` - `hybrid_search()`, `index_episode_embeddings()`
- `api/search.py` - `/api/search/semantic` endpoint
- `mcp/tools.py` - `semantic_search` MCP tool
- `worker/manager.py` - Embedding worker (processes `EMBED` jobs)

### API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/search/semantic?q={query}&mode={mode}` | Hybrid search (mode: hybrid/semantic/keyword) |
| `GET /api/search/semantic/stats` | Embedding statistics |

### MCP Tool

```python
semantic_search(query="protein and muscle building", mode="hybrid")
```

### Reindexing

```bash
# Reindex FTS only
cast2md reindex-transcripts

# Reindex FTS and regenerate embeddings (needed after model change)
cast2md reindex-transcripts --embeddings
```

### Startup Behavior

- **Embeddings**: Persisted in PostgreSQL (survives restarts)
- **Model loading**: ~3 seconds on first semantic query after restart
- **Background worker**: Automatically generates embeddings for new transcripts

### pgvector Notes

- Uses HNSW index for fast approximate nearest neighbor search
- Vector column defined as `vector(384)` matching embedding dimension
- Cosine distance used for similarity: `embedding <=> query_embedding`

## UI Guidelines

### Tooltips

Always use custom CSS tooltips instead of native browser `title` attributes. Native tooltips are unreliable across browsers and have inconsistent display timing.

Implementation pattern (see `base.html`):
```css
.element-with-tooltip {
    position: relative;
    cursor: help;
}
.element-with-tooltip::after {
    content: attr(title);
    position: absolute;
    bottom: 100%;
    left: 50%;
    transform: translateX(-50%);
    padding: 0.4rem 0.6rem;
    background: var(--pico-card-background-color);
    color: var(--pico-color);
    font-size: 0.75rem;
    border-radius: 4px;
    white-space: nowrap;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.15s, visibility 0.15s;
    z-index: 1000;
    pointer-events: none;
}
.element-with-tooltip:hover::after {
    opacity: 1;
    visibility: visible;
}
```

The `title` attribute still holds the tooltip text (for accessibility), but CSS `::after` with `content: attr(title)` renders it visually.

## RunPod Afterburner

On-demand GPU transcription worker for processing large backlogs. Uses **Parakeet TDT 0.6B v3** by default for fast transcription (supports 25 European languages including German).

### Quick Start

```bash
source deploy/afterburner/.env
python deploy/afterburner/afterburner.py --dry-run  # Validate config
python deploy/afterburner/afterburner.py --test     # Test connectivity
python deploy/afterburner/afterburner.py            # Process queue
```

### Key Files

- `deploy/afterburner/afterburner.py` - Main script (installs NeMo toolkit for Parakeet)
- `deploy/afterburner/startup.sh` - Reference copy of startup script
- `deploy/afterburner/.env.example` - Environment configuration template

### Transcription Models

RunPod pods default to Parakeet but can use Whisper models. Models are configurable via the RunPod settings page:

- **Manage Models**: Add/remove models in "Manage Transcription Models" section
- **Custom Models**: Add any Whisper or Parakeet model by ID
- **API**: `GET/POST/DELETE /api/runpod/models`

Default models:
- `parakeet-tdt-0.6b-v3` - Fast, 25 EU languages (default)
- `large-v3-turbo`, `large-v3`, `large-v2`, `medium`, `small` - Whisper models

### Node Worker Prefetch

The node worker uses a **3-slot prefetch queue** to keep audio ready for instant transcription. This is important for Parakeet which transcribes faster than download speed.

### Tailscale Userspace Networking (Important Lessons)

RunPod containers don't have `/dev/net/tun`, so Tailscale must run in **userspace mode**. This has significant implications:

#### 1. No TUN Interface

```bash
# This is required - can't use default TUN mode
tailscaled --tun=userspace-networking --state=/var/lib/tailscale/tailscaled.state
```

#### 2. Inbound Connections Work Normally

Tailscale SSH works fine because `tailscaled` handles incoming connections:

```bash
# This works once pod is on Tailnet
ssh root@<pod-hostname>
```

#### 3. Outbound Connections Need HTTP Proxy

Applications can't directly connect to Tailscale IPs. Must use the HTTP proxy:

```bash
# Start tailscaled WITH the proxy
tailscaled --tun=userspace-networking --outbound-http-proxy-listen=localhost:1055 &

# Use proxy for outbound traffic
curl -x http://localhost:1055 http://100.x.x.x:8000/api/health

# Or set environment variable
http_proxy=http://localhost:1055 some-command
```

#### 4. HTTP Proxy Doesn't Support HTTPS CONNECT

**Critical limitation**: The proxy only handles plain HTTP. HTTPS fails:

```bash
# Works (HTTP)
curl -x http://localhost:1055 http://server:8000/api/health

# Fails (HTTPS - no CONNECT tunneling)
curl -x http://localhost:1055 https://server/api/health
```

**Solution**: Use HTTP on port 8000 for internal Tailscale traffic. It's still encrypted by Tailscale's WireGuard tunnel.

#### 5. MagicDNS Not Available

`*.ts.net` hostnames don't resolve in userspace mode. Must use `/etc/hosts`:

```bash
echo '100.x.x.x server.tailnet.ts.net' >> /etc/hosts
```

This is why `CAST2MD_SERVER_IP` environment variable is required.

#### 6. Pod Detection with Multiple Orphaned Hosts

Tailscale keeps offline hosts visible. When hostname is taken, it adds `-1`, `-2` suffixes. The script handles this by:

1. Matching hostname prefix (`runpod-afterburner*`)
2. Filtering for `Online=true`
3. Sorting by `Created` timestamp (newest first)
4. Verifying SSH connectivity before proceeding

### Parallel Execution

The CLI supports parallel execution. Run multiple instances simultaneously:

```bash
# Each generates unique instance ID (e.g., "a3f2")
python deploy/afterburner/afterburner.py &
python deploy/afterburner/afterburner.py &
python deploy/afterburner/afterburner.py &

# Terminate all
python deploy/afterburner/afterburner.py --terminate-all
```

### Debugging Tips

```bash
# Check if proxy is listening
ssh root@<pod-ip> "ss -tlnp | grep 1055"

# Test proxy connectivity
ssh root@<pod-ip> "curl -x http://localhost:1055 http://<server-ip>:8000/api/health"

# Check Tailscale status
ssh root@<pod-ip> "tailscale status"

# View node worker logs
ssh root@<pod-ip> "tail -100 /tmp/cast2md-node.log"
```

### Server-Side RunPod Management

The server includes a RunPod service for managing GPU workers via API. This enables future admin UI integration.

#### Enabling Server-Side RunPod

1. Install optional dependency: `pip install cast2md[runpod]`
2. Set environment variables:
   ```bash
   RUNPOD_API_KEY=...           # Required
   RUNPOD_TS_AUTH_KEY=...       # Required (Tailscale auth key)
   RUNPOD_SERVER_URL=https://<your-tailnet>
   RUNPOD_SERVER_IP=100.x.x.x   # Tailscale IP
   ```
3. Enable in settings: `runpod_enabled=true`

#### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/runpod/status` | GET | Status, active pods, setup states |
| `/api/runpod/pods` | POST | Create new pod (async) |
| `/api/runpod/pods/{instance_id}/setup-status` | GET | Track pod creation progress |
| `/api/runpod/pods` | DELETE | Terminate all pods |
| `/api/runpod/pods/{pod_id}` | DELETE | Terminate specific pod |
| `/api/runpod/pods/{instance_id}/persistent` | PATCH | Set dev mode (persistent) on/off |
| `/api/runpod/pods/{instance_id}/update-code` | POST | Update code on running pod (dev mode) |
| `/api/runpod/setup-states/{instance_id}` | DELETE | Dismiss a setup state |

#### Dev Mode

For development and debugging, pods can be created in **persistent mode** which:
- Prevents auto-termination after processing
- Allows updating code without recreating the pod
- Persists setup state across server restarts

**Enabling dev mode on a running pod:**

```bash
# Set dev mode on (prevents auto-termination, allows code updates)
curl -X PATCH https://<your-tailnet>/api/runpod/pods/{instance_id}/persistent \
  -H "Content-Type: application/json" -d '{"persistent": true}'

# Disable dev mode
curl -X PATCH https://<your-tailnet>/api/runpod/pods/{instance_id}/persistent \
  -H "Content-Type: application/json" -d '{"persistent": false}'
```

Dev mode is useful for:
- Testing code changes on a live pod
- Debugging transcription issues
- Extended monitoring

**Updating code on a running pod:**

```bash
# Via API
curl -X POST https://<your-tailnet>/api/runpod/pods/{instance_id}/update-code

# What it does:
# 1. Stops the worker process
# 2. Reinstalls cast2md from git (pip install --no-cache-dir)
# 3. Restarts the worker with correct environment (TRANSCRIPTION_BACKEND, WHISPER_MODEL)
```

This avoids the ~2 minute overhead of recreating the entire pod for each code change.

**Setup state persistence:**

Pod setup states are stored in the database (`pod_setup_states` table) and survive server restarts. This means:
- Pods created before a restart are still tracked after restart
- Failed states can be dismissed via the API
- Persistent pods remain visible in the status UI

#### Key Files

- `src/cast2md/services/runpod_service.py` - Pod lifecycle management
- `src/cast2md/api/runpod.py` - REST API endpoints
- `src/cast2md/config/settings.py` - RunPod settings (runpod_* prefix)

#### Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `runpod_enabled` | `false` | Master switch |
| `runpod_max_pods` | `3` | Max concurrent pods |
| `runpod_auto_scale` | `false` | Auto-start on queue growth |
| `runpod_scale_threshold` | `10` | Queue depth to trigger auto-scale |
| `runpod_gpu_type` | `NVIDIA RTX A5000` | Preferred GPU |
| `runpod_blocked_gpus` | `NVIDIA GeForce RTX 4090,NVIDIA GeForce RTX 4080,NVIDIA L4` | Comma-separated GPU blocklist |
| `runpod_whisper_model` | `parakeet-tdt-0.6b-v3` | Transcription model for pods |

#### GPU Compatibility

**Important:** RTX 40-series consumer GPUs and certain datacenter GPUs have CUDA compatibility issues with NeMo/Parakeet, causing `CUDA error 35` during transcription. These GPUs work fine with Whisper but fail with Parakeet.

**Working GPUs for Parakeet:**
- NVIDIA RTX A5000 (~$0.20-0.25/hr, ~87x realtime)
- NVIDIA RTX A6000
- NVIDIA RTX A4000
- NVIDIA GeForce RTX 3090
- NVIDIA L40

**Blocked GPUs (default blocklist):**
- NVIDIA GeForce RTX 4090
- NVIDIA GeForce RTX 4080
- NVIDIA L4

The blocklist is applied during pod creation and fallback selection. Blocked GPUs are automatically skipped. To modify:

```bash
# Add to .env or systemd environment
runpod_blocked_gpus="NVIDIA GeForce RTX 4090,NVIDIA GeForce RTX 4080,NVIDIA L4"
```
